import streamlit as st
import os
import hashlib
import torch
import torchaudio
from pydub import AudioSegment
import io
import whisper
from speechbrain.inference import EncoderClassifier
from difflib import SequenceMatcher
from pinyin_pro import pinyin  # æ–°å¢æ‹¼éŸ³è½¬æ¢åº“

# ================== è§†é¢‘æµå¼æ’­æ”¾æ¨¡å— ==================
def video_player():
    """åœ¨é¡µé¢é¡¶éƒ¨è‡ªåŠ¨æ’­æ”¾å¼•å¯¼è§†é¢‘"""
    try:
        # æ’­æ”¾æœ¬åœ°MP4æ–‡ä»¶ï¼ˆéœ€é™éŸ³è‡ªåŠ¨æ’­æ”¾ï¼‰
        with open("./video/nezha2.mp4", "rb") as f:
            video_bytes = f.read()
            st.video(video_bytes, 
                    autoplay=True, 
                    muted=True,  # ç»•è¿‡æµè§ˆå™¨è‡ªåŠ¨æ’­æ”¾é™åˆ¶
                    start_time=0,
                    format="video/mp4")
    except FileNotFoundError:
        st.warning("å¼•å¯¼è§†é¢‘æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")

# ================== æ ¸å¿ƒåŠŸèƒ½æ¨¡å— ==================
@st.cache_resource
def load_models():
    """åŠ è½½è¯­éŸ³å¤„ç†æ¨¡å‹"""
    spk_classifier = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        savedir="pretrained_models"
    )
    
    # é¢„åŠ è½½åŸºå‡†å£°çº¹
    base_signal, fs = torchaudio.load(BASE_AUDIO_PATH)
    if fs != 16000:
        base_signal = torchaudio.functional.resample(base_signal, fs, 16000)
    base_emb = spk_classifier.encode_batch(base_signal).squeeze(0).flatten()
    
    return spk_classifier, whisper.load_model("base"), base_emb

def pinyin_compare(target, transcript):
    """å¢å¼ºç‰ˆæ‹¼éŸ³å¯¹æ¯”åŠŸèƒ½"""
    # è½¬æ¢ä¸ºæ— éŸ³è°ƒæ‹¼éŸ³æ•°ç»„
    target_py = pinyin(target, tonetype='none', type='array')
    trans_py = pinyin(transcript, tonetype='none', type='array')
    
    # è®¡ç®—ä¸¤ç§ç›¸ä¼¼åº¦
    char_sim = SequenceMatcher(None, target, transcript).ratio()
    pinyin_sim = SequenceMatcher(None, "".join(target_py), "".join(trans_py)).ratio()
    
    return {
        "characters": char_sim,
        "pinyin": pinyin_sim,
        "target_py": " ".join(target_py),
        "trans_py": " ".join(trans_py)
    }

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    MAX_FILE_SIZE = 10 * 1024 * 1024
    TEMP_DIR = "./temp_uploads"
    BASE_AUDIO_PATH = "./video/ä»™ç¿çº¯äº«.mp3"
    TARGET_TEXT = "ç½‘ç»œå®‰å…¨é˜²æŠ¤é‡äºæ³°å±±"
    os.makedirs(TEMP_DIR, exist_ok=True)

    # åˆå§‹åŒ–ç•Œé¢
    st.set_page_config(page_title="CTFå£°çº¹éªŒè¯ç³»ç»Ÿ", layout="wide")
    
    # é¡¶éƒ¨è§†é¢‘æ’­æ”¾
    video_player()
    
    # ä¸»æ ‡é¢˜
    st.title("CTFå£°çº¹éªŒè¯ç³»ç»Ÿ v5.0")
    
    # ç´ æä¸‹è½½åŒº
    with st.expander("ğŸ”½ éªŒè¯ç´ æä¸‹è½½"):
        col1, col2 = st.columns(2)
        with col1:
            with open("./video/nezha2.mp4", "rb") as f:
                st.download_button("ä¸‹è½½å“ªå’ç´ æ", f, "nezha2.mp4", "video/mp4")
        with col2:
            with open(BASE_AUDIO_PATH, "rb") as f:
                st.download_button("ä¸‹è½½åŸºå‡†éŸ³é¢‘", f, "ä»™ç¿çº¯äº«.mp3", "audio/mpeg")

    # åŠ è½½æ¨¡å‹
    spk_model, asr_model, BASE_EMBEDDING = load_models()

    # éªŒè¯è¡¨å•
    with st.form("verify_form"):
        uploaded_file = st.file_uploader("ä¸Šä¼ éªŒè¯è¯­éŸ³", type=["mp3", "wav"])
        
        if st.form_submit_button("å¼€å§‹éªŒè¯"):
            if not uploaded_file:
                st.warning("è¯·ä¸Šä¼ è¯­éŸ³æ–‡ä»¶")
                st.stop()
                
            if uploaded_file.size > MAX_FILE_SIZE:
                st.error("æ–‡ä»¶å¤§å°è¶…è¿‡10MBé™åˆ¶")
                st.stop()

            try:
                # é¢„å¤„ç†éŸ³é¢‘
                conv_path = os.path.join(TEMP_DIR, f"temp_{hashlib.md5(uploaded_file.getvalue()).hexdigest()}.wav")
                AudioSegment.from_file(io.BytesIO(uploaded_file.getvalue()))\
                    .set_frame_rate(16000)\
                    .set_channels(1)\
                    .export(conv_path, format="wav")

                # å¹¶è¡Œå¤„ç†éªŒè¯æµç¨‹
                col1, col2 = st.columns(2)
                results = {}
                
                with col1:
                    with st.spinner("ğŸ” å£°çº¹åˆ†æä¸­..."):
                        signal, fs = torchaudio.load(conv_path)
                        if fs != 16000:
                            signal = torchaudio.functional.resample(signal, fs, 16000)
                        user_emb = spk_model.encode_batch(signal).squeeze(0).flatten()
                        results["voice_sim"] = torch.nn.functional.cosine_similarity(
                            BASE_EMBEDDING.unsqueeze(0), 
                            user_emb.unsqueeze(0), 
                            dim=1
                        ).item()
                
                with col2:
                    with st.spinner("ğŸ”¤ å†…å®¹è¯†åˆ«ä¸­..."):
                        transcript = asr_model.transcribe(
                            conv_path,
                            language="zh",
                            initial_prompt="ä»¥ä¸‹æ˜¯æ ‡å‡†æ™®é€šè¯å¥å­"
                        )["text"].strip()
                        results.update(pinyin_compare(TARGET_TEXT, transcript))

                # å±•ç¤ºéªŒè¯ç»“æœ
                st.subheader("éªŒè¯ç»“æœ")
                
                # å£°çº¹éªŒè¯éƒ¨åˆ†
                with st.expander("ğŸ—£ï¸ å£°çº¹éªŒè¯è¯¦æƒ…"):
                    col_a, col_b = st.columns(2)
                    with col_a:
                        st.metric("å£°çº¹ç›¸ä¼¼åº¦", 
                                f"{results['voice_sim']:.2%}",
                                delta="â‰¥85%" if results['voice_sim'] >= 0.85 else None)
                    with col_b:
                        st.plotly_chart(px.bar(
                            x=["ç›¸ä¼¼åº¦"],
                            y=[results['voice_sim']],
                            range_y=[0, 1],
                            title="å£°çº¹åŒ¹é…åº¦å¯è§†åŒ–"
                        ))
                
                # æ–‡æœ¬éªŒè¯éƒ¨åˆ†
                with st.expander("ğŸ“ æ–‡æœ¬éªŒè¯è¯¦æƒ…"):
                    tab1, tab2, tab3 = st.tabs(["åŸå§‹å¯¹æ¯”", "æ‹¼éŸ³å¯¹æ¯”", "ç›¸ä¼¼åº¦åˆ†æ"])
                    
                    with tab1:
                        st.write(f"**ç›®æ ‡æ–‡æœ¬**: {TARGET_TEXT}")
                        st.write(f"**è¯†åˆ«ç»“æœ**: {transcript}")
                        st.code(f"å­—ç¬¦åŒ¹é…åº¦: {results['characters']:.2%}")
                    
                    with tab2:
                        st.write(f"**ç›®æ ‡æ‹¼éŸ³**: {results['target_py']}")
                        st.write(f"**è¯†åˆ«æ‹¼éŸ³**: {results['trans_py']}")
                        st.code(f"æ‹¼éŸ³åŒ¹é…åº¦: {results['pinyin']:.2%}")
                    
                    with tab3:
                        st.plotly_chart(px.pie(
                            values=[results['characters'], results['pinyin']],
                            names=["å­—ç¬¦åŒ¹é…", "æ‹¼éŸ³åŒ¹é…"],
                            title="ç»¼åˆåŒ¹é…åˆ†æ"
                        ))

                # æœ€ç»ˆåˆ¤å®š
                if results['voice_sim'] >= 0.85 and results['characters'] >= 0.7:
                    flag = hashlib.sha256(conv_path.encode()).hexdigest()[:16]
                    st.success(f"âœ… éªŒè¯æˆåŠŸ! FLAG: CTF{{VoiceAuth_{flag}}}")
                    st.balloons()
                else:
                    st.error("âŒ éªŒè¯å¤±è´¥ï¼šæœªé€šè¿‡åŒé‡æ ¡éªŒ")
                
            except Exception as e:
                st.error(f"ç³»ç»Ÿé”™è¯¯: {str(e)}")
            finally:
                if os.path.exists(conv_path):
                    os.remove(conv_path)