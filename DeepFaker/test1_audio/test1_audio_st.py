import streamlit as st
import os
import hashlib
import torch
import torchaudio
from pydub import AudioSegment
import io
import whisper
from speechbrain.inference import EncoderClassifier
from difflib import SequenceMatcher



# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
print(f"å½“å‰è„šæœ¬ç›®å½•: {SCRIPT_DIR}")
# ä¿®æ”¹åçš„è·¯å¾„é…ç½®
BASE_AUDIO_PATH = os.path.join(SCRIPT_DIR, "video", "ä»™ç¿çº¯äº«.mp3")
TEMP_DIR = os.path.join(SCRIPT_DIR, "temp_uploads")
VIDEO_PATH = os.path.join(SCRIPT_DIR, "video", "nezha2.mp4")
os.makedirs(TEMP_DIR, exist_ok=True)


# å®‰å…¨é…ç½®
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
@st.cache_resource
def load_models():
    """åŠ è½½è¯­éŸ³æ¨¡å‹å¹¶è¿”å›"""
    # å£°çº¹è¯†åˆ«æ¨¡å‹
    spk_classifier = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        savedir="pretrained_models"
    )
    
    # è¯­éŸ³è¯†åˆ«æ¨¡å‹
    asr_model = whisper.load_model("small")
    
    # é¢„åŠ è½½åŸºå‡†å£°çº¹ï¼ˆç»´åº¦ä¿®å¤å…³é”®ï¼‰
    base_signal, fs = torchaudio.load(BASE_AUDIO_PATH)
    if base_signal.shape[0] > 1:
        base_signal = torch.mean(base_signal, dim=0, keepdim=True)
    if fs != 16000:
        base_signal = torchaudio.functional.resample(base_signal, fs, 16000)
    base_emb = spk_classifier.encode_batch(base_signal)
    base_emb = base_emb.squeeze(0).flatten()  # ç»´åº¦è°ƒæ•´ [1, 192] -> [192]
    
    return spk_classifier, asr_model, base_emb

spk_model, asr_model, BASE_EMBEDDING = load_models()

def audio_preprocessing(uploaded_file):
    """ä¼˜åŒ–éŸ³é¢‘é¢„å¤„ç†æµç¨‹"""
    try:
        audio = AudioSegment.from_file(io.BytesIO(uploaded_file.getvalue()))
        
        # å¢å¼ºé¢„å¤„ç†ï¼šå»é™¤é™éŸ³æ®µ
        processed = (audio.set_frame_rate(16000)
                    .set_channels(1)
                    .normalize())
        
        # ä¿å­˜é¢„å¤„ç†æ–‡ä»¶
        converted_path = os.path.join(TEMP_DIR, f"conv_{hashlib.md5(uploaded_file.getvalue()).hexdigest()}.wav")
        processed.export(converted_path, format="wav")
        return converted_path
    except Exception as e:
        raise RuntimeError(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {str(e)}")

def extract_voiceprint(audio_path):
    """ç»´åº¦ä¿®å¤åçš„å£°çº¹ç‰¹å¾æå–"""
    try:
        signal, fs = torchaudio.load(audio_path)
        if signal.shape[0] > 1:
            signal = torch.mean(signal, dim=0, keepdim=True)
        if fs != 16000:
            signal = torchaudio.functional.resample(signal, fs, 16000)
        
        # ç»´åº¦å¤„ç†æµç¨‹ä¼˜åŒ–
        embeddings = spk_model.encode_batch(signal)
        return embeddings.squeeze(0).flatten()  # è¾“å‡ºå½¢çŠ¶ [192]
    except Exception as e:
        raise RuntimeError(f"å£°çº¹æå–å¤±è´¥: {str(e)}")

def verify_speaker(user_emb):
    """ä¿®å¤åçš„ç›¸ä¼¼åº¦è®¡ç®—"""
    try:
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        if user_emb.dim() == 1:
            user_emb = user_emb.unsqueeze(0)
        
        similarity = torch.nn.functional.cosine_similarity(
            BASE_EMBEDDING.unsqueeze(0), 
            user_emb, 
            dim=1  # ä¿®æ­£ç»´åº¦å‚æ•°
        )
        return max(similarity.item(), 0.0)
    except Exception as e:
        raise RuntimeError(f"å£°çº¹éªŒè¯å¤±è´¥: {str(e)}")

def transcribe_audio(audio_path):
    """å¢å¼ºç‰ˆè¯­éŸ³è½¬æ–‡å­—"""
    try:
        result = asr_model.transcribe(
            audio_path,
            language="zh",
            initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯å¥å­",
            fp16=False
        )
        return result["text"].strip()
    except Exception as e:
        raise RuntimeError(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")

# ç½‘é¡µç•Œé¢
st.title("å…«å¦å®«è¯­éŸ³éªŒè¯ v1.0")
TARGET_TEXT = "æˆ‘ä¹ƒæ— é‡ä»™ç¿ å¸ˆå¼Ÿåˆ«æ¥æ— æ™"  # å›ºå®šéªŒè¯æ–‡æœ¬

video_file = open(VIDEO_PATH, 'rb')
video_bytes = video_file.read()
st.video(video_bytes, format="video/mp4") 

# ç´ æä¸‹è½½åŒº
st.markdown("### ğŸ§ ç´ æä¸‹è½½")
col1, col2 = st.columns(2)

with col1:
    with open(VIDEO_PATH, "rb") as f:
        st.download_button(
            label="ä¸‹è½½å“ªå’ç´ æ",
            data=f,
            file_name="nezha2.mp4",
            mime="video/mp4",
            help="åŒ…å«å£°çº¹ç‰¹å¾çš„å‚è€ƒè§†é¢‘"
        )

with col2:
    with open(BASE_AUDIO_PATH, "rb") as f:
        st.download_button(
            label="ä¸‹è½½åŸºå‡†éŸ³é¢‘",
            data=f,
            file_name="ä»™ç¿çº¯äº«.mp3",
            mime="audio/mpeg",
            help="ç›®æ ‡å£°çº¹çš„åŸå§‹å½•éŸ³"
        )

# éªŒè¯ç›®æ ‡å£°æ˜
st.markdown(f"""
<div style="background:#f0f2f6;padding:20px;border-radius:10px">
    <h4>ğŸ¯ éªŒè¯ç›®æ ‡è¦æ±‚</h4>
    <p>1. ä½¿ç”¨<b>ä»™ç¿å£°çº¹ç‰¹å¾</b>å½•åˆ¶è¯­éŸ³ï¼ˆç›¸ä¼¼åº¦â‰¥é˜ˆå€¼%ï¼‰</p>
    <p>2. æ¸…æ™°è¯»å‡ºä»¥ä¸‹æ–‡æœ¬ï¼š<code>{TARGET_TEXT}</code>ï¼ˆåŒ¹é…åº¦â‰¥70%ï¼‰</p>
</div>
""", unsafe_allow_html=True)

# éªŒè¯è¡¨å•
with st.form("verify_form"):
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ éªŒè¯è¯­éŸ³ï¼ˆMP3/WAVï¼‰",
        type=["mp3", "wav"],
        help="æ–‡ä»¶éœ€åŒæ—¶æ»¡è¶³å£°çº¹å’Œæ–‡æœ¬è¦æ±‚"
    )
    
    if st.form_submit_button("å¼€å§‹éªŒè¯"):
        if not uploaded_file:
            st.warning("è¯·å…ˆä¸Šä¼ è¯­éŸ³æ–‡ä»¶")
            st.stop()
            
        if uploaded_file.size > MAX_FILE_SIZE:
            st.error("æ–‡ä»¶å¤§å°è¶…è¿‡10MBé™åˆ¶")
            st.stop()

        try:
            # é¢„å¤„ç†é˜¶æ®µ
            conv_path = audio_preprocessing(uploaded_file)
            
            # éŸ³é¢‘é•¿åº¦éªŒè¯
            audio_length = AudioSegment.from_file(conv_path).duration_seconds
            if audio_length < 2.0:
                st.error("âŒ è¯­éŸ³æ—¶é•¿ä¸è¶³2ç§’ï¼Œè¯·é‡æ–°å½•åˆ¶")
                st.stop()
            
            # å¹¶è¡Œå¤„ç†åŒéªŒè¯
            col1, col2 = st.columns(2)
            
            with col1:
                with st.spinner("ğŸ” å£°çº¹åˆ†æä¸­..."):
                    user_emb = extract_voiceprint(conv_path)
                    sim_score = verify_speaker(user_emb)
                    
            with col2:
                with st.spinner("ğŸ”¤ æ–‡æœ¬è¯†åˆ«ä¸­..."):
                    transcript = transcribe_audio(conv_path)
                    text_match = SequenceMatcher(
                        None, 
                        TARGET_TEXT, 
                        transcript
                    ).ratio()
            
            # æ˜¾ç¤ºéªŒè¯ç»“æœ
            st.subheader("éªŒè¯ç»“æœ")
            rs_col1, rs_col2 = st.columns(2)
            
            with rs_col1:
                st.metric("å£°çº¹ç›¸ä¼¼åº¦", 
                         f"{sim_score+(1-sim_score)*0.5:.2%}", #æ­£åˆ™åŒ–å£°çº¹ç›¸ä¼¼åº¦
                         delta="â‰¥75%" if sim_score >= 0.5 else None)
                
            with rs_col2:
                st.metric("æ–‡æœ¬åŒ¹é…åº¦",
                         f"{text_match:.2%}",
                         delta="â‰¥70%" if text_match >= 0.7 else None)
            
            st.divider()
            st.subheader("è¯†åˆ«å†…å®¹")
            st.code(transcript)
            st.caption(f"ç›®æ ‡æ–‡æœ¬: {TARGET_TEXT}")
            
            # æœ€ç»ˆåˆ¤å®š
            if sim_score >= 0.5 and text_match >= 0.7:
                flag = hashlib.sha256(conv_path.encode()).hexdigest()[:16]
                st.success(f"âœ… éªŒè¯é€šè¿‡! FLAG: CTF{{VoiceAuth_{flag}}}")
                st.balloons()
            else:
                st.error("âŒ éªŒè¯å¤±è´¥ï¼šæœªé€šè¿‡åŒé‡æ ¡éªŒ")
                
        except Exception as e:
            st.error(f"ç³»ç»Ÿé”™è¯¯: {str(e)}")
        finally:
            if os.path.exists(conv_path):
                os.remove(conv_path)